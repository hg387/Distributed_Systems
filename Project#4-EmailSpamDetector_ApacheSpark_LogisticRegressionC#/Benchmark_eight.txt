hgupt004@cloudshell:~ (vocal-nova-316714)$ gcloud dataproc jobs submit pyspark gs://cs647_hw4/main.py --cluster=clustereight --region=us-east4
Job [5486b9fc669a4948bc9b54dfd3cea4b5] submitted.
Waiting for job output...
21/06/17 19:52:05 INFO org.sparkproject.jetty.util.log: Logging initialized @4244ms to org.sparkproject.jetty.util.log.Slf4jLog
21/06/17 19:52:05 INFO org.sparkproject.jetty.server.Server: jetty-9.4.36.v20210114; built: 2021-01-14T16:44:28.689Z; git: 238ec6997c7806b055319a6d11f8ae7564adc0de; jvm 1.8.0_292-b10
21/06/17 19:52:05 INFO org.sparkproject.jetty.server.Server: Started @4364ms
21/06/17 19:52:05 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@5eba6f8d{HTTP/1.1, (http/1.1)}{0.0.0.0:45183}
21/06/17 19:52:06 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at clustereight-m/10.150.0.6:8032
21/06/17 19:52:06 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at clustereight-m/10.150.0.6:10200
21/06/17 19:52:07 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
21/06/17 19:52:07 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
21/06/17 19:52:09 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1623959370612_0001
21/06/17 19:52:11 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at clustereight-m/10.150.0.6:8030
Starting training data time
21/06/17 19:52:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1500
21/06/17 19:52:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1500
21/06/17 19:52:17 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #0,5,main]) interrupted:
java.lang.InterruptedException
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)
        at com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)
        at org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)
        at org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
21/06/17 19:53:21 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 3672
21/06/17 19:53:21 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #19,5,main]) interrupted:
java.lang.InterruptedException
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)
        at com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)
        at org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)
        at org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
21/06/17 19:53:22 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 3672
21/06/17 19:54:56 WARN org.apache.spark.scheduler.TaskSetManager: Stage 4 contains a task of very large size (1981 KiB). The maximum recommended task size is 1000 KiB.
21/06/17 19:54:57 WARN org.apache.spark.scheduler.TaskSetManager: Stage 5 contains a task of very large size (1981 KiB). The maximum recommended task size is 1000 KiB.
21/06/17 19:55:08 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1500
21/06/17 19:55:08 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #19,5,main]) interrupted:
java.lang.InterruptedException
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)
        at com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)
        at org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)
        at org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
21/06/17 19:55:09 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1500
21/06/17 19:55:49 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 3672
21/06/17 19:55:50 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 3672
21/06/17 19:57:22 WARN org.apache.spark.scheduler.TaskSetManager: Stage 10 contains a task of very large size (1981 KiB). The maximum recommended task size is 1000 KiB.
21/06/17 19:57:23 WARN org.apache.spark.scheduler.TaskSetManager: Stage 11 contains a task of very large size (1981 KiB). The maximum recommended task size is 1000 KiB.
Finished training data in 350.3614761829376 seconds
Starting classifying data time
21/06/17 19:58:06 WARN org.apache.spark.scheduler.TaskSetManager: Stage 12 contains a task of very large size (3302 KiB). The maximum recommended task size is 1000 KiB.
21/06/17 19:58:07 WARN org.apache.spark.scheduler.TaskSetManager: Stage 13 contains a task of very large size (3302 KiB). The maximum recommended task size is 1000 KiB.
spam files classified correctly: 1179
spam files classified_incorrectly: 321
21/06/17 19:58:08 WARN org.apache.spark.scheduler.TaskSetManager: Stage 14 contains a task of very large size (9011 KiB). The maximum recommended task size is 1000 KiB.
21/06/17 19:58:09 WARN org.apache.spark.scheduler.TaskSetManager: Stage 15 contains a task of very large size (9011 KiB). The maximum recommended task size is 1000 KiB.
non-spam files classified correctly: 3107
non-spam files classified incorrectly: 565
Finished training data time 4.464009761810303 seconds
21/06/17 19:58:10 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@5eba6f8d{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
Job [5486b9fc669a4948bc9b54dfd3cea4b5] finished successfully.
done: true
driverControlFilesUri: gs://dataproc-staging-us-east4-516411995044-aj9nd6sq/google-cloud-dataproc-metainfo/acb53ccf-463f-49b2-b2c4-a2c194ef8c16/jobs/5486b9fc669a4948bc9b54dfd3cea4b5/
driverOutputResourceUri: gs://dataproc-staging-us-east4-516411995044-aj9nd6sq/google-cloud-dataproc-metainfo/acb53ccf-463f-49b2-b2c4-a2c194ef8c16/jobs/5486b9fc669a4948bc9b54dfd3cea4b5/driveroutput
jobUuid: 205aeef7-17a6-34e4-b54f-c3d87fe69b7b
placement:
  clusterName: clustereight
  clusterUuid: acb53ccf-463f-49b2-b2c4-a2c194ef8c16
pysparkJob:
  mainPythonFileUri: gs://cs647_hw4/main.py
reference:
  jobId: 5486b9fc669a4948bc9b54dfd3cea4b5
  projectId: vocal-nova-316714
status:
  state: DONE
  stateStartTime: '2021-06-17T19:58:14.591601Z'
statusHistory:
- state: PENDING
  stateStartTime: '2021-06-17T19:51:59.317071Z'
- state: SETUP_DONE
  stateStartTime: '2021-06-17T19:51:59.382692Z'
- details: Agent reported job success
  state: RUNNING
  stateStartTime: '2021-06-17T19:51:59.699998Z'
yarnApplications:
- name: CS647_HW4
  progress: 1.0
  state: FINISHED
  trackingUrl: http://clustereight-m:8088/proxy/application_1623959370612_0001/